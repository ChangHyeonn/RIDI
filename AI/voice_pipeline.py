import sys
import os
import time
import logging
from typing import Dict, Any

# AI ëª¨ë“ˆ import
sys.path.append(os.path.join(os.path.dirname(__file__), 'Models'))
from Models.STT import WhisperSTT
from Models.LLM import LLMFactory
from Models.TTS import TTS

class VoicePipeline:
    """STT â†’ LLM â†’ TTS ìŒì„± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                 stt_model: str = "small",
                 llm_type: str = "gemini",  # "gpt" or "gemini"
                 device: str = "auto"):
        self.device = self._get_device(device)
        self.llm_type = llm_type
        self._setup_logging()
        self._initialize_components(stt_model)
        self.logger.info(f"Voice Pipeline initialized successfully on {self.device}")
    
    def _get_device(self, device: str) -> str:
        if device == "auto":
            import torch
            # CUDA í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
            if torch.cuda.is_available():
                return "cuda"
            # CPU (MPS í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ ê¸°ë³¸ê°’)
            else:
                return "cpu"
        return device
    
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _initialize_components(self, stt_model: str):
        self.logger.info("Initializing AI components...")
        
        self.stt = WhisperSTT(model_name=stt_model, device=self.device)
        self.stt.optimize_for_korean(True)
        
        # LLM ì´ˆê¸°í™” (GPT/Gemini ì„ íƒ ê°€ëŠ¥)
        self.llm = LLMFactory.create_llm(self.llm_type)
        
        self.tts = TTS()
        
        self.logger.info("All AI components initialized")
    
    def process_voice_input(self, audio_path: str) -> Dict[str, Any]:
        """
        ìŒì„± ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # Step 1: STT (ìŒì„± â†’ í…ìŠ¤íŠ¸)
            transcribed_text = self._process_stt(audio_path)
            if not transcribed_text:
                return self._create_error_response("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # Step 2: LLM (í…ìŠ¤íŠ¸ â†’ ì‘ë‹µ)
            llm_response = self._process_llm(transcribed_text)
            
            # Step 3: TTS (í…ìŠ¤íŠ¸ â†’ ìŒì„±)
            audio_output = self._process_tts(llm_response)
            
            total_time = time.time() - start_time
            
            return self._create_success_response(
                transcribed_text, llm_response, audio_output, total_time
            )
            
        except Exception as e:
            self.logger.error(f"Pipeline processing failed: {e}")
            return self._create_error_response(str(e))
    
    def _process_stt(self, audio_path: str) -> str:
        """STT ì²˜ë¦¬"""
        self.logger.info("Processing STT...")
        transcribed_text = self.stt.transcribe(audio_path, use_preprocessing=True)
        self.logger.info(f"STT ê²°ê³¼: {transcribed_text}")
        return transcribed_text
    
    def _process_llm(self, text: str) -> str:
        """LLM ì²˜ë¦¬"""
        self.logger.info("Processing LLM...")
        llm_response = self.llm.generate_response(text)
        self.logger.info(f"LLM ì‘ë‹µ: {llm_response}")
        return llm_response
    
    def _process_tts(self, text: str) -> bytes:
        """TTS ì²˜ë¦¬"""
        self.logger.info("Processing TTS...")
        audio_output = self.tts.generate_from_llm_response(text)
        return audio_output
    
    def _create_success_response(self, transcribed_text: str, llm_response: str, 
                               audio_output: bytes, total_time: float) -> Dict[str, Any]:
        """ì„±ê³µ ì‘ë‹µ ìƒì„±"""
        return {
            "success": True,
            "transcribed_text": transcribed_text,
            "llm_response": llm_response,
            "audio_output": audio_output,
            "total_time": round(total_time, 2)
        }
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return {
            "success": False,
            "error": error_message,
            "transcribed_text": None,
            "llm_response": None,
            "audio_output": None
        }
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì •ë³´ ë°˜í™˜"""
        return {
            "pipeline_name": "STT â†’ LLM â†’ TTS",
            "device": self.device,
            "components": {
                "stt": self.stt.get_model_info(),
                "llm": self.llm.get_model_info(),
                "tts": self.tts.get_model_info()
            }
        }
    
    def __del__(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'stt'):
            del self.stt
        if hasattr(self, 'llm'):
            del self.llm
        if hasattr(self, 'tts'):
            del self.tts

def main():
    """ë©”ì¸ í•¨ìˆ˜ - íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤ Voice Pipeline í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (device ìë™ ì„ íƒ)
    pipeline = VoicePipeline(
        stt_model="small",
        llm_model="meta-llama/Llama-3-8B-Instruct",
        use_quantization=True,
        device="auto"  # ìë™ìœ¼ë¡œ MPS/CUDA/CPU ì„ íƒ
    )
    
    # íŒŒì´í”„ë¼ì¸ ì •ë³´ ì¶œë ¥
    pipeline_info = pipeline.get_pipeline_info()
    print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ì •ë³´:")
    print(f"  ì´ë¦„: {pipeline_info['pipeline_name']}")
    print(f"  Device: {pipeline_info['device']}")
    print(f"  STT ëª¨ë¸: {pipeline_info['components']['stt']['model_name']}")
    print(f"  LLM ëª¨ë¸: {pipeline_info['components']['llm']['model_name']}")
    print(f"  TTS ëª¨ë¸: {pipeline_info['components']['tts']['model_name']}")
    
    # í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ë“¤
    test_files = [
        "test_audio.wav",
        "sample_voice.mp3", 
        "recording.m4a",
        "voice_sample.wav"
    ]
    
    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
    audio_file = None
    for file in test_files:
        if os.path.exists(file):
            audio_file = file
            break
    
    if not audio_file:
        print("\nâŒ í…ŒìŠ¤íŠ¸ìš© ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”:")
        for file in test_files:
            print(f"  - {file}")
        print("\në˜ëŠ” ì§ì ‘ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
        audio_file = input("íŒŒì¼ ê²½ë¡œ: ").strip()
        
        if not audio_file or not os.path.exists(audio_file):
            print("âŒ ìœ íš¨í•œ ìŒì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print(f"\nğŸ¯ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {audio_file}")
    result = pipeline.process_voice_input(audio_file)
    
    if result["success"]:
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
        print(f"ğŸ“ STT ê²°ê³¼: {result['transcribed_text']}")
        print(f"ğŸ¤– LLM ì‘ë‹µ: {result['llm_response']}")
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {result['total_time']}ì´ˆ")
    else:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result['error']}")

if __name__ == "__main__":
    main() 